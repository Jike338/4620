---
title: "4620 Project"
author: "Jike Zhong"
date: "11/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R}
library("glmnet")
library(stringr)
library(GGally)
library("tidyr")
library("dplyr")
```

#load data
```{R}

data_train <- read.csv("train.csv")
data_test <- read.csv("test_new.csv")



data_train <- data_train %>% mutate_if(is.numeric, ~replace_na(., 0))
data_train <- data_train %>% mutate_if(is.character, ~replace_na(., "No"))

data_test <- data_test %>% mutate_if(is.numeric, ~replace_na(., 0))
data_test <- data_test %>% mutate_if(is.character, ~replace_na(., "No"))

print(dim(data_train))
print(dim(data_test))

# str(data_train)
#sapply(lapply(data_train, unique), length)
```

#Motivation 
Housing market is an important sector of the economy. Having an accurate prediction of housing price is of interest to the general public and the economic forecast. Conventional models for predictions include regression, decision trees, naive bayes, recurrent neural networks, etc. A good model should be able to generalize well to the test data, meaning that it should aim to capture the global minimum (in a strong convex optimization problem) without over-fitting or under-fitting, this means we need to take into account the bias-variance trade-off. Motivated by these insights, we propose using Lasso (L1 regularization) based logistic regression model for this particular task. Fitting a lasso-based regression model should ensure enough model capacity while minimizing the chance of over-fitting through regularization. It also has advantage over neural network given our limited amount of data samples. To find the best modeling strategy, we also test Ridge along with Lasso. 
#Math
#Assumption
#Validation
Here we explain one important step to the data augmentation. For fairness purposes, we need to ensure that our train and test distributions are the same (e.g. predictors existing in test data must also exist in train data). For this reason, we first compute the confusion matrix of both the train and test data, we then drop the features that are in the disjoint set of train and test sets. 

#Result
We evaluate our models by mean squared errorand our result shows that Lasso out-performs Ridge.
```{R}
x_train <-  model.matrix(SalePrice~ . -1 , data = data_train)
y_train <-  data_train$SalePrice

x_test <-  model.matrix(SalePrice~ . -1 , data = data_test)
y_test <-  data_test$SalePrice


print(dim(x_train))
print(dim(x_test))

#x_train-x_test
missing_cols = c()
for (var in colnames(x_train)){
  if (!(var %in% colnames(x_test))){
    missing_cols <- c(missing_cols, var)
  }
}

#x_test-x_train
missing_cols_2 = c()
for (var in colnames(x_test)){
  if (!(var %in% colnames(x_train))){
    missing_cols_2 <- c(missing_cols_2, var)
  }
}

#we simply remove the mismatch to ensure the same distribution between train and test dataset
x_train <- x_train[, !colnames(x_train) %in% missing_cols]
x_test <- x_test[, !colnames(x_test) %in% missing_cols_2]
```

#lasso
```{R}
cv.lasso <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 1)
#plot(cv.ridge)
model =glmnet(x_train,y_train,lambda=cv.lasso$lambda.min, alpha=1)

#lasso.bestlam =cv.lasso$lambda.min
#tmp_coef = coef(cv.lasso,s=lasso.bestlam)
#varnames = data.frame(name = tmp_coef@Dimnames[[1]][tmp_coef@i])
#mylist = list(name = tmp_coef@Dimnames[[1]][tmp_coef@i])

pred <- predict(model,x_test)
mean((pred-y_test)^2)
```

#ridge
```{R}
cv.ridge <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0)
#plot(cv.ridge)
model =glmnet(x_train,y_train,lambda=cv.ridge$lambda.min, alpha=0)
pred <- predict(model,x_test)
mean((pred-y_test)^2)
```

```{R}
pls.fit=plsr(Apps~., data=train, scale=TRUE, validation="CV")
pls.pred=predict(pls.fit, test, ncomp = 7)
mean((pls.pred-test$Apps)^2)
```
